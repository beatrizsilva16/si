{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Exercício 8: Adiciona o método randomized_search_cv.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "O método randomized_search_cv implementa uma estratégia de otimização de parâmetros de usando Nº combinações aleatórias. O randomized_search_cv avalia apenas um conjunto aleatório de parâmetros retirados de uma distribuição ou conjunto de valores possíveis"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "8.2) Valida a tua implementação seguindo o protocolo:\n",
    "1. Usa o dataset breast-bin.csv\n",
    "2. Usa o sklearn.preprocessing.StandardScaler para standardizar os dataset.\n",
    "breast_dataset.X = StandardScaler().fit_transform(breast_dataset.X)\n",
    "3. Cria o modelo LogisticRegression\n",
    "4. Realiza uma procura aleatória com as seguintes distribuições de parâmetros:\n",
    "• l2_penalty: distribuição entre 1 e 10 com 10 intervalos iguais (e.g., np.linspace(1, 10, 10))\n",
    "• alpha: distribuição entre 0.001 e 0.0001 com 100 intervalos iguais (e.g., np.linspace(0.001, 0.0001, 100))\n",
    "• max_iter: distribuição entre 1000 e 2000 com 200 intervalos iguais (e.g., np.linspace(1000, 2000, 200))\n",
    "5. Podes usar n_iter de 10 e 3 folds para o cross_validate.\n",
    "6. Quais os scores obtidos?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cross validation scores: {'seeds': [762, 22, 562], 'train': [0.964221824686941, 0.9695885509838997, 0.9677996422182469], 'test': [0.97841726618705, 0.9568345323741004, 0.964028776978417]}\n"
     ]
    }
   ],
   "source": [
    "from si.io_folder.csv_file import read_csv\n",
    "from si.model_selection.cross_validate import cross_validate\n",
    "from si.linear_model.logistic_regression import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Exercise 1: Test the cross_validate using the breast-bin.csv dataset\n",
    "\n",
    "# (1)\n",
    "breast = read_csv('C:/Users/beatr/Mestrado/2ºano/si/datasets/breast-bin.csv', sep=',', features=True, label=True)\n",
    "\n",
    "# (2) Standardize features by removing the mean and scaling to unit variance.\n",
    "breast.X = StandardScaler().fit_transform(breast.X)\n",
    "\n",
    "# (3) Create Logistic Regression model\n",
    "lr = LogisticRegression(l2_penalty= 1, alpha= 0.001, max_iter=2000)\n",
    "\n",
    "# (4) Performs a cross validation with 5 folds\n",
    "scores = cross_validate(lr, breast, cv=3)\n",
    "\n",
    "# (5) What is the score obtained?\n",
    "print(f\"The cross validation scores: {scores}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scores: {'seeds': [166, 536, 93], 'train': [0.9695885509838997, 0.9695885509838998, 0.9660107334525938], 'test': [0.9568345323741005, 0.9568345323741004, 0.9712230215827335]}\n"
     ]
    }
   ],
   "source": [
    "from si.io_folder.csv_file import read_csv\n",
    "from si.model_selection.cross_validate import cross_validate\n",
    "from si.linear_model.logistic_regression import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# - Exercise 2: Test the grid_search using the breast-bin.csv dataset\n",
    "\n",
    "# (1) Read csv file - breast-bin.csv\n",
    "breast = read_csv('C:/Users/beatr/Mestrado/2ºano/si/datasets/breast-bin.csv', sep=',', features=True, label=True)\n",
    "\n",
    "# (2) Standardize features by removing the mean and scaling to unit variance.\n",
    "breast.X = StandardScaler().fit_transform(breast.X)\n",
    "\n",
    "# (3) Create Logistic Regression model\n",
    "lr2 = LogisticRegression(l2_penalty= 1, alpha= 0.001, max_iter=2000)\n",
    "\n",
    "# (4) Perform a grid search with the following parameters\n",
    "lr2_param = {'l2_penalty': [1, 10], 'alpha': [0.001, 0.0001], 'max_iter': [1000, 2000]}\n",
    "\n",
    "# (5) cross validate the model with 3 folds\n",
    "scores = cross_validate(lr2, breast, cv=3)\n",
    "\n",
    "# (6) What is the score obtained?\n",
    "print(f\"The scores: {scores}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-1-38db1f7e4b3c>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[1;31m# (5.1) cross validate the model with 3 folds\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 23\u001B[1;33m \u001B[0mscores_3\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mrandomized_search_cv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlr3\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbreast\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlr3_param\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcv\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     24\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     25\u001B[0m \u001B[1;31m# (5.2) cross validate the model with 10 folds\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Mestrado\\2ºano\\si\\src\\si\\model_selection\\randomized_search_cv.py\u001B[0m in \u001B[0;36mrandomized_search_cv\u001B[1;34m(model, dataset, parameter_distribution, scoring, cv, n_iter, test_size)\u001B[0m\n\u001B[0;32m     48\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     49\u001B[0m         \u001B[1;31m# performs cross_validation with the combination\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 50\u001B[1;33m         \u001B[0mscore\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcross_validate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mscoring\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mscoring\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcv\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcv\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtest_size\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     51\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     52\u001B[0m         \u001B[1;31m# stores the parameter combination and the obtained scores\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Mestrado\\2ºano\\si\\src\\si\\model_selection\\cross_validate.py\u001B[0m in \u001B[0;36mcross_validate\u001B[1;34m(model, dataset, scoring, cv, test_size)\u001B[0m\n\u001B[0;32m     37\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     38\u001B[0m         \u001B[1;31m# fit the model on the train set\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 39\u001B[1;33m         \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     40\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     41\u001B[0m         \u001B[1;31m# score the model on the test set\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Mestrado\\2ºano\\si\\src\\si\\linear_model\\logistic_regression.py\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, dataset)\u001B[0m\n\u001B[0;32m     50\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     51\u001B[0m         \u001B[1;31m# gradient descent\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 52\u001B[1;33m         \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmax_iter\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     53\u001B[0m             \u001B[1;31m# predicted y\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     54\u001B[0m             \u001B[0my_pred\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdot\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtheta\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtheta_zero\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: 'numpy.float64' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from si.io_folder.csv_file import read_csv\n",
    "from si.linear_model.logistic_regression import LogisticRegression\n",
    "from si.model_selection.randomized_search_cv import randomized_search_cv\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# - Exercise 8: Test the randomized_search_cv using the breast-bin.csv dataset\n",
    "\n",
    "# (1) Read csv file - breast-bin.csv\n",
    "breast = read_csv('C:/Users/beatr/Mestrado/2ºano/si/datasets/breast-bin.csv', sep=',', features=True, label=True)\n",
    "\n",
    "# (2) Standardize features by removing the mean and scaling to unit variance.\n",
    "breast.X = StandardScaler().fit_transform(breast.X)\n",
    "\n",
    "# (3) Create Logistic Regression model\n",
    "lr3 = LogisticRegression(l2_penalty= 1, alpha= 0.001, max_iter=2000)\n",
    "\n",
    "# (4) Perform a randomized search with the following parameters\n",
    "lr3_param = {'l2_penalty': np.linspace(1,10, num=10), 'alpha': np.linspace( 0.001, 0.0001, 100),\n",
    "             'max_iter': np.linspace(1000, 2000, 200)}\n",
    "\n",
    "# (5.1) cross validate the model with 3 folds\n",
    "scores_3 = randomized_search_cv(lr3, breast, lr3_param, cv=3)\n",
    "\n",
    "# (5.2) cross validate the model with 10 folds\n",
    "scores_10 = randomized_search_cv(lr3, breast, lr3_param, cv=10)\n",
    "\n",
    "# (6) What is the score obtained?\n",
    "print(f\"The scores with 3 folds: {scores_3}\")\n",
    "print(f\"The scores with 10 folds: {scores_10}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}